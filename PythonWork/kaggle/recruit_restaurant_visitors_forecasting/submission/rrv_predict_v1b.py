#!/usr/bin/python
# -*- coding: utf-8 -*-

import numpy as np 
import pandas as pd 
import re
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cross_validation import train_test_split
import xgboost as xgb

import urllib2
from sklearn.model_selection import StratifiedKFold

from sklearn import ensemble, neighbors, linear_model, metrics, preprocessing
from datetime import datetime
import glob, re

def  main():
    train_raw_data_air_store_info = pd.read_csv("../input/air_store_info.csv")
    train_raw_data_air_reserve = pd.read_csv("../input/air_reserve.csv")
    train_raw_data_air_visit_data=pd.read_csv("../input/air_visit_data.csv")
    train_raw_data_date_info = pd.read_csv("../input/date_info.csv")

    train_raw_data_hpg_reserve = pd.read_csv("../input/hpg_reserve.csv")
    train_raw_data_hpg_store_info = pd.read_csv("../input/hpg_store_info.csv")
    
    train_raw_data_store_id_relation = pd.read_csv("../input/store_id_relation.csv")


    # missing_df = np.sum(train_raw_data==-1, axis=0)

    # print train_raw_data_air_store_info.isnull().sum()
    # print train_raw_data_air_reserve.isnull().sum()
    # print train_raw_data_air_visit_data.isnull().sum()
    # print train_raw_data_date_info.isnull().sum()
    # print train_raw_data_hpg_store_info.isnull().sum()
    # print train_raw_data_hpg_reserve.isnull().sum()
    # print train_raw_data_store_id_relation.isnull().sum()

    # from JdPaletto & the1owl1
    # JdPaletto - https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503?scriptVersionId=1867420
    # the1owl1 - https://www.kaggle.com/the1owl/surprise-me

    data = {
        'tra': pd.read_csv('../input/air_visit_data.csv'),
        'as': pd.read_csv('../input/air_store_info.csv'),
        'hs': pd.read_csv('../input/hpg_store_info.csv'),
        'ar': pd.read_csv('../input/air_reserve.csv'),
        'hr': pd.read_csv('../input/hpg_reserve.csv'),
        'id': pd.read_csv('../input/store_id_relation.csv'),
        'tes': pd.read_csv('../input/sample_submission.csv'),
        'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})
        }

    data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])

    for df in ['ar','hr']:
        data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])
        data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date
        data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])
        data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date
        data[df]['reserve_datetime_diff'] = data[df].apply(
            lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)
        data[df] = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[[
            'reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date'})

    data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])
    data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek
    data['tra']['year'] = data['tra']['visit_date'].dt.year
    data['tra']['month'] = data['tra']['visit_date'].dt.month
    data['tra']['visit_date'] = data['tra']['visit_date'].dt.date

    data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])
    data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))
    data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])
    data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek
    data['tes']['year'] = data['tes']['visit_date'].dt.year
    data['tes']['month'] = data['tes']['visit_date'].dt.month
    data['tes']['visit_date'] = data['tes']['visit_date'].dt.date

    unique_stores = data['tes']['air_store_id'].unique()
    stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': 
        [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)

    tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)[
        'visitors'].min().rename(columns={'visitors':'min_visitors'})
    stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) 
    tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)[
        'visitors'].mean().rename(columns={'visitors':'mean_visitors'})
    stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])
    tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)[
        'visitors'].median().rename(columns={'visitors':'median_visitors'})
    stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])
    tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)[
        'visitors'].max().rename(columns={'visitors':'max_visitors'})
    stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])
    tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)[
        'visitors'].count().rename(columns={'visitors':'count_observations'})
    stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) 

    stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) 
    lbl = preprocessing.LabelEncoder()
    stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])
    stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])

    data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])
    data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])
    data['hol']['visit_date'] = data['hol']['visit_date'].dt.date
    train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) 
    test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) 

    train = pd.merge(data['tra'], stores, how='left', on=['air_store_id','dow']) 
    test = pd.merge(data['tes'], stores, how='left', on=['air_store_id','dow'])

    for df in ['ar','hr']:
        train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) 
        test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])

    col = [c for c in train if c not in ['id', 'air_store_id','visit_date','visitors']]
    train = train.fillna(-1)
    test = test.fillna(-1)

    def RMSLE(y, pred):
        return metrics.mean_squared_error(y, pred)**0.5

    etc = ensemble.ExtraTreesRegressor(n_estimators=225, max_depth=5, n_jobs=-1, random_state=3)
    knn = neighbors.KNeighborsRegressor(n_jobs=-1, n_neighbors=4)
    etc.fit(train[col], np.log1p(train['visitors'].values))
    knn.fit(train[col], np.log1p(train['visitors'].values))

    test['visitors'] = (etc.predict(test[col]) / 2) +(knn.predict(test[col]) / 2)
    test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)
    sub1 = test[['id','visitors']].copy()


    # from hklee
    # https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code
    dfs = { re.search('/([^/\.]*)\.csv', fn).group(1):
        pd.read_csv(fn)for fn in glob.glob('../input/*.csv')}

    for k, v in dfs.items(): 
        locals()[k] = v

    wkend_holidays = date_info.apply(
        (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)
    date_info.loc[wkend_holidays, 'holiday_flg'] = 0
    date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  

    visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')
    visit_data.drop('calendar_date', axis=1, inplace=True)
    visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)

    wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )
    visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()
    visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.

    sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))
    sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])
    sample_submission.drop('visitors', axis=1, inplace=True)
    sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')
    sample_submission = sample_submission.merge(visitors, on=[
        'air_store_id', 'day_of_week', 'holiday_flg'], how='left')

    missings = sample_submission.visitors.isnull()
    sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(
        visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), 
        how='left')['visitors_y'].values

    missings = sample_submission.visitors.isnull()
    sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(
        visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), 
        on='air_store_id', how='left')['visitors_y'].values
        
    sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)
    sub2 = sample_submission[['id', 'visitors']].copy()
    sub_merge = pd.merge(sub1, sub2, on='id', how='inner')

    ## Arithmetric Mean 
    sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y'])/2
    sub_merge[['id', 'visitors']].to_csv('sub_math_mean.csv', index=False)

    ## Geometric Mean  
    sub_merge['visitors'] = (sub_merge['visitors_x'] * sub_merge['visitors_y']) ** (1/2)
    sub_merge[['id', 'visitors']].to_csv('sub_geo_mean.csv', index=False)

    ## Harmonic Mean 
    sub_merge['visitors'] = 2/(1/sub_merge['visitors_x'] + 1/sub_merge['visitors_y'])
    sub_merge[['id', 'visitors']].to_csv('sub_hrm_mean.csv', index=False)




if __name__=="__main__":
    main()